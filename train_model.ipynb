{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "'Num words: 7 - max sentence length 78'"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(\"data/dataset_sentence_level.csv\")\n",
    "print(len(dataset_df))\n",
    "dataset_df.head()\n",
    "all_sentences = dataset_df['sentence'].values.tolist()\n",
    "all_words = ['<PAD>', '<SOS>', '<EOS>'] #including special tokens\n",
    "\n",
    "all_splitted_sentences = []\n",
    "max_len = 0\n",
    "for sentences in all_sentences:\n",
    "    words = sentences.split()\n",
    "    if len(words) > max_len:\n",
    "        max_len = len(words)\n",
    "    all_splitted_sentences.append(words)\n",
    "    [all_words.append(w) for w in words if w not in all_words]\n",
    "\n",
    "vocab = {w: idx for idx, w in enumerate(all_words)}\n",
    "\n",
    "pad_idx = vocab['<PAD>']\n",
    "sos_idx = vocab['<SOS>']\n",
    "eos_idx = vocab['<EOS>']\n",
    "f\"Num words: {len(words)} - max sentence length {max_len}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "\n",
    "all_input_idx = []\n",
    "all_target_idx = []\n",
    "all_sequence_len = []\n",
    "for i, sentence in enumerate(all_splitted_sentences):\n",
    "    word_idx = [vocab[w] for w in sentence]\n",
    "    # including <eos> and <sos> tokens\n",
    "    input_idx = [sos_idx] + word_idx\n",
    "    target_idx = word_idx + [eos_idx]\n",
    "    # padding both sequences\n",
    "    pad_len = (max_len + 1) - len(word_idx)\n",
    "    pad_input_idx = input_idx + ([pad_idx] * pad_len)\n",
    "    pad_target_idx = target_idx + ([pad_idx] * pad_len)\n",
    "\n",
    "    all_input_idx.append(pad_input_idx)\n",
    "    all_target_idx.append(pad_target_idx)\n",
    "\n",
    "    all_sequence_len.append(len(input_idx))\n",
    "\n",
    "tensor_input = torch.tensor(all_input_idx, device=device)\n",
    "tensor_output = torch.tensor(all_target_idx, device=device)\n",
    "tensor_len = torch.tensor(all_sequence_len, device=device)\n",
    "dataset = TensorDataset(tensor_input, tensor_output, tensor_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autoencoder\n",
    "- Incluir <SOS> token no dado de treino\n",
    "- A inferência é autoregressiva, SOS serve para dar o start no processo iterativo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "categorical_dim = 10  # one-of-K vector\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    return -Variable(torch.log(-torch.log(U + eps) + eps))\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"\n",
    "    ST-gumple-softmax\n",
    "    input: [*, n_class]\n",
    "    return: flatten --> [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    print(f\"gumbel input size {logits.size()}\")\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    print(f\"gumbel size {y.size()}\")\n",
    "\n",
    "    if not hard:\n",
    "        print(f\"gumbel final size {y.view(-1, latent_dim * categorical_dim).size()}\")\n",
    "        return y.view(-1, latent_dim * categorical_dim)\n",
    "\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "    y_hard = (y_hard - y).detach() + y\n",
    "\n",
    "    return y_hard.view(-1, latent_dim * categorical_dim)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # encoder\n",
    "        self.embedding = nn.Embedding(embedding_dim=100, num_embeddings=len(vocab), padding_idx=pad_idx)\n",
    "        self.lstm_encoder = nn.GRU(batch_first=True, hidden_size=100, input_size=100, bidirectional=True)\n",
    "        # VAE\n",
    "        self.lstm_decoder = nn.GRU(batch_first=True, hidden_size=100, input_size=100)\n",
    "        self.gumbel_input = nn.Linear(200, latent_dim*categorical_dim)\n",
    "        # converte o z em um vetor para ser usado como h_t no decoder lstm\n",
    "        self.z_embedding = nn.Linear(latent_dim * categorical_dim, 100)\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=100, out_features=len(vocab)) #\n",
    "\n",
    "    def encode(self, x, seq_len):\n",
    "        x_emb = self.embedding(x)\n",
    "        x_pack = pack_padded_sequence(x_emb, seq_len.numpy(), batch_first=True)\n",
    "        print(type(x_pack))\n",
    "        x, ht = self.lstm_encoder(x_pack)\n",
    "        encoded_sequence = ht.view(ht.size(1), ht.size(2) * 2) # bidirectional -> + <-\n",
    "        return x, encoded_sequence, x_pack\n",
    "\n",
    "    def decoder(self, input, z, max_seq_len):\n",
    "        z_emb = self.z_embedding(z).unsqueeze(0) # z_emb será usado como h inicial do LSTM decoder\n",
    "        print(f\"z_emb {z_emb.size()}\")\n",
    "        #x = input + z_emb - educating text-autoencoder\n",
    "        #z_emb = z_emb.view(-1, 2, 100)\n",
    "        print(f\"z_emb after view {z_emb.size()}\")\n",
    "        hidden = (z_emb, z_emb) # h_t, c_t\n",
    "        x, (h_t, c_t) = self.lstm_decoder(input, hidden)\n",
    "        # TODO: incluir tamanho max da sequencia original para calcular o loss\n",
    "        x, _ = pad_packed_sequence(x, batch_first=True, total_length=max_seq_len)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, seq_len, temperature):\n",
    "        # ordering by sequence length\n",
    "        sorted_lengths, sorted_idx = torch.sort(seq_len, descending=True)\n",
    "        x = x[sorted_idx]\n",
    "\n",
    "        batch_size, max_seq_len, _ = x.size() # [batch_size, maximum seq_len from current batch, dim]\n",
    "\n",
    "        x, encoded_sequence, x_pack = self.encode(x, sorted_lengths)\n",
    "        q_y = self.gumbel_input(encoded_sequence)\n",
    "        z = gumbel_softmax(q_y, temperature=temperature)\n",
    "        x = self.decoder(x_pack, z, max_seq_len)\n",
    "        return x, q_y\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [262], line 22\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m#x = x.unsqueeze(-1)\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m#y = y.unsqueeze(-1)\u001B[39;00m\n\u001B[1;32m     21\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 22\u001B[0m y_hat, qy \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_lens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function(y\u001B[38;5;241m=\u001B[39my, y_hat\u001B[38;5;241m=\u001B[39my_hat, qy\u001B[38;5;241m=\u001B[39mqy)\n\u001B[1;32m     24\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn [260], line 77\u001B[0m, in \u001B[0;36mAutoencoder.forward\u001B[0;34m(self, x, seq_len, temperature)\u001B[0m\n\u001B[1;32m     74\u001B[0m sorted_lengths, sorted_idx \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msort(seq_len, descending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     75\u001B[0m x \u001B[38;5;241m=\u001B[39m x[sorted_idx]\n\u001B[0;32m---> 77\u001B[0m batch_size, max_seq_len, _ \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;66;03m# [batch_size, maximum seq_len from current batch, dim]\u001B[39;00m\n\u001B[1;32m     79\u001B[0m x, encoded_sequence, x_pack \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode(x, sorted_lengths)\n\u001B[1;32m     80\u001B[0m q_y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgumbel_input(encoded_sequence)\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "def loss_function(y, y_hat, qy):\n",
    "    recon_loss = nn.CrossEntropyLoss(y, y_hat)\n",
    "\n",
    "    qy_softmax = F.softmax(qy)\n",
    "    log_ratio = torch.log(qy_softmax * categorical_dim - 1e-20)\n",
    "    KLD = torch.sum(qy * log_ratio, dim=-1).sum()\n",
    "    return recon_loss + KLD\n",
    "\n",
    "model = Autoencoder()\n",
    "train_dataloader = DataLoader(dataset, batch_size=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "temp = 1.\n",
    "temp_min = 0.5\n",
    "ANNEAL_RATE = 0.00003\n",
    "\n",
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    x, y, seq_lens = batch\n",
    "    optimizer.zero_grad()\n",
    "    y_hat, qy = model(x, seq_lens, temperature=temp)\n",
    "    loss = loss_function(y=y, y_hat=y_hat, qy=qy)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % 100 == 1:\n",
    "        temp = np.maximum(temp * np.exp(-ANNEAL_RATE * batch_idx), temp_min)\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([20, 20, 1])"
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(20,20).unsqueeze(-1).size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "data": {
      "text/plain": "-2.3025850929940455"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "np.log(1/categorical_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-2.2991, grad_fn=<MeanBackward0>)"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KLD = torch.sum(q_y * log_ratio, dim=-1).mean()\n",
    "KLD"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# in this case\n",
    "- log equivalence: log(x) - log (y) = log(x/y)\n",
    "- $$log(q_y) - log(\\frac{1}{N}) = log(\\frac{q_y}{\\frac{1}{N}}) = log(q_y * N)$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-230.6127, grad_fn=<SumBackward0>)"
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_qy = torch.log(q_y)\n",
    "g = torch.log(torch.tensor([1/categorical_dim])) # log(q_y) - log(1/cat_dim)\n",
    "(log_qy - g).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-230.6127, grad_fn=<SumBackward0>)"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_ratio = torch.log(q_y * categorical_dim + 1e-20) # log(q_y * 1/cat_dim)\n",
    "log_ratio.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 5]) torch.Size([1, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [1, 5], got [1, 3]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [276], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m target \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(\u001B[38;5;241m3\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\u001B[38;5;241m.\u001B[39mrandom_(\u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(), target\u001B[38;5;241m.\u001B[39msize())\n\u001B[0;32m----> 6\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m output\n",
      "File \u001B[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1166\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   3012\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3013\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3014\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected target size [1, 5], got [1, 3]"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "input = torch.randn(3, 5, requires_grad=True).unsqueeze(0)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5).unsqueeze(0)\n",
    "print(input.size(), target.size())\n",
    "output = loss(input, target)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536, 160]) torch.Size([1536])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(5.5871)"
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(96, 16, 160)\n",
    "y = torch.empty(96, 16, dtype=torch.long).random_(160)\n",
    "print(x.view(-1, 160).size(), y.view(-1).size())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(x.view(-1, 160), y.view(-1))\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}