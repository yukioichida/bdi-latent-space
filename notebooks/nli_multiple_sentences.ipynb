{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-09-03T15:35:36.541912Z",
     "end_time": "2023-09-03T15:35:37.629198Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import itertools\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model\n",
      "Combinations = 2\n",
      "{'entailment': 0, 'neutral': 1, 'contradiction': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded 6.353092193603516 - model cpu\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Loading Model\")\n",
    "\n",
    "# AIMA -> explicar no contexto de busca\n",
    "obs = ['This room is called the foundry.']\n",
    "\n",
    "context = ['you are in workshop', 'you are in foundry']\n",
    "#\n",
    "\n",
    "a = list(itertools.product(obs, context))\n",
    "a.sort(key=lambda x: x[1])\n",
    "print(f\"Combinations = {len(a)}\")\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "hg_model_hub_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "#hg_model_hub_name = \"alisawuffles/roberta-large-wanli\"\n",
    "config = AutoConfig.from_pretrained(hg_model_hub_name)\n",
    "entailment_idx = config.label2id['entailment']\n",
    "\n",
    "config = AutoConfig.from_pretrained(hg_model_hub_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hg_model_hub_name)\n",
    "\n",
    "print(config.label2id)\n",
    "\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(hg_model_hub_name)\n",
    "model.to(device)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Model loaded {end - start} - model {model.device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-03T15:35:38.967810Z",
     "end_time": "2023-09-03T15:35:45.323873Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.28208255767822266\n",
      "----\n",
      "Premise: This room is called the foundry.\n",
      "Hypothesis: you are in foundry\n",
      "Entailment: 0.9029870629310608\n",
      "Neutral: 0.09309663623571396\n",
      "Contradiction: 0.003916335757821798\n",
      "----\n",
      "Premise: This room is called the foundry.\n",
      "Hypothesis: you are in workshop\n",
      "Entailment: 0.5612775683403015\n",
      "Neutral: 0.3587671220302582\n",
      "Contradiction: 0.07995528727769852\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tokenized_input_seq_pair = tokenizer.batch_encode_plus(a,\n",
    "                                                       max_length=max_length,\n",
    "                                                       return_token_type_ids=True, truncation=True, padding=True)\n",
    "\n",
    "input_ids = torch.tensor(tokenized_input_seq_pair['input_ids'], device=device).long()  #.unsqueeze(0)\n",
    "# remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.\n",
    "token_type_ids = torch.tensor(tokenized_input_seq_pair['token_type_ids'], device=device).long()  #.unsqueeze(0)\n",
    "attention_mask = torch.tensor(tokenized_input_seq_pair['attention_mask'], device=device).long()  #.unsqueeze(0)\n",
    "\n",
    "outputs = model(input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=None)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Inference time: {end - start}\")\n",
    "logits = outputs[0]\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "for i, pair in enumerate(a):\n",
    "    predicted_probability = probs[i].tolist()  # batch_size only one\n",
    "    print('----')\n",
    "    print(\"Premise:\", pair[0])\n",
    "    print(\"Hypothesis:\", pair[1])\n",
    "    print(\"Entailment:\", predicted_probability[int(config.label2id['entailment'])])\n",
    "    print(\"Neutral:\", predicted_probability[int(config.label2id['neutral'])])\n",
    "    print(\"Contradiction:\", predicted_probability[int(config.label2id['contradiction'])])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-03T15:35:45.329767Z",
     "end_time": "2023-09-03T15:35:45.666985Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- $context = (c_1, c_2)$\n",
    "- $beliefbase = (b_1, b_2)$\n",
    "- $(b_1 \\models c_1 \\lor b_2 \\models c_1) \\land (b_1 \\models c_2 \\lor b_2 \\models c_2)$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([1, 2, 2, 2, 2, 2, 2])]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_context = len(context)\n",
    "num_beliefs = len(obs)\n",
    "\n",
    "argmax_probs = probs.argmax(-1)\n",
    "slice_idx = []\n",
    "idx = 0\n",
    "for i in range(num_context):\n",
    "    slice_idx.append(argmax_probs[idx:(idx + num_beliefs)])\n",
    "    idx = num_beliefs\n",
    "slice_idx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T17:33:56.103784Z",
     "end_time": "2023-08-31T17:33:56.107310Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#c1_or = torch.where(c1 == entailment_idx, True, False)\n",
    "#c2_or = torch.where(c2 == entailment_idx, True, False)\n",
    "\n",
    "context_or = [torch.where(c == entailment_idx, True, False).any().unsqueeze(0) for c in slice_idx]\n",
    "all_ors = torch.concatenate(context_or)  # contiguous\n",
    "entailment = all_ors.all()\n",
    "if entailment:\n",
    "    print(entailment.item())\n",
    "else:\n",
    "    print(\"not entailment\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T17:33:56.110306Z",
     "end_time": "2023-08-31T17:33:56.113811Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.9644, grad_fn=<MeanBackward0>)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True when a c_n is entailed by b_n\n",
    "entailment_mask = torch.where(argmax_probs == entailment_idx, True, False)\n",
    "# [B,c1:B,c2:...:B,cn]\n",
    "slice_idx = []\n",
    "idx = 0\n",
    "for i in range(num_context):  # [c1, ..., cn]\n",
    "    slice_idx.append(argmax_probs[idx:(idx + num_beliefs)])\n",
    "    idx = num_beliefs\n",
    "\n",
    "# True if ANY context comparation is ENTAILED by an belief in belief base (OR)\n",
    "context_or = [torch.where(c == entailment_idx, True, False).any().unsqueeze(0) for c in slice_idx]\n",
    "or_tensor = torch.concatenate(context_or)  # at least one belief should entail a context (OR)\n",
    "and_result = or_tensor.all()  # all context must be entailed by the belief base (AND)\n",
    "entailment_result = and_result.item()  # boolean result\n",
    "\n",
    "entailment_probs = (probs[:, entailment_idx] * entailment_mask)  # retrieving only entailment predictions\n",
    "entailment_score = entailment_probs[entailment_probs != 0].mean()  # mean of all entailment predictions\n",
    "entailment_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T17:33:56.116836Z",
     "end_time": "2023-08-31T17:33:56.122876Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 2, 2, 2, 2, 2])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax_probs[argmax_probs == 2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T17:33:56.125781Z",
     "end_time": "2023-08-31T17:33:56.185334Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entailment_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m entailment_probs \u001B[38;5;241m=\u001B[39m (probs[:, entailment_idx] \u001B[38;5;241m*\u001B[39m \u001B[43mentailment_mask\u001B[49m)\n\u001B[1;32m      2\u001B[0m entailment_probs[entailment_probs \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmean()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'entailment_mask' is not defined"
     ]
    }
   ],
   "source": [
    "entailment_probs = (probs[:, entailment_idx] * entailment_mask)\n",
    "entailment_probs[entailment_probs != 0].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T15:52:32.611493Z",
     "end_time": "2023-07-10T15:52:32.616407Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Tensor"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(entailment_probs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T15:56:26.398385Z",
     "end_time": "2023-07-10T15:56:26.407458Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
