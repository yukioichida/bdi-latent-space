{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23110\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BeliefAutoencoder:\n\tMissing key(s) in state_dict: \"lstm_encoder.weight_ih_l0\", \"lstm_encoder.weight_hh_l0\", \"lstm_encoder.bias_ih_l0\", \"lstm_encoder.bias_hh_l0\", \"lstm_encoder.weight_ih_l0_reverse\", \"lstm_encoder.weight_hh_l0_reverse\", \"lstm_encoder.bias_ih_l0_reverse\", \"lstm_encoder.bias_hh_l0_reverse\", \"lstm_decoder.weight_ih_l0\", \"lstm_decoder.weight_hh_l0\", \"lstm_decoder.bias_ih_l0\", \"lstm_decoder.bias_hh_l0\". \n\tUnexpected key(s) in state_dict: \"encoder.weight_ih_l0\", \"encoder.weight_hh_l0\", \"encoder.bias_ih_l0\", \"encoder.bias_hh_l0\", \"encoder.weight_ih_l0_reverse\", \"encoder.weight_hh_l0_reverse\", \"encoder.bias_ih_l0_reverse\", \"encoder.bias_hh_l0_reverse\", \"decoder.weight_ih_l0\", \"decoder.weight_hh_l0\", \"decoder.bias_ih_l0\", \"decoder.bias_hh_l0\", \"decoder.weight_ih_l0_reverse\", \"decoder.weight_hh_l0_reverse\", \"decoder.bias_ih_l0_reverse\", \"decoder.bias_hh_l0_reverse\". \n\tsize mismatch for sampling_input.weight: copying a param with shape torch.Size([16, 256]) from checkpoint, the shape in current model is torch.Size([16, 512]).\n\tsize mismatch for z_embedding.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n\tsize mismatch for z_embedding.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [34], line 24\u001B[0m\n\u001B[1;32m     20\u001B[0m model \u001B[38;5;241m=\u001B[39m BeliefAutoencoder(emb_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, h_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m, latent_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, vocab\u001B[38;5;241m=\u001B[39mvocab, categorical_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbc\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m#model.load_state_dict(torch.load(\"models/belief-autoencoder-bc-YP68fCFu.pth\", map_location='cpu'))\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m#model.load_state_dict(torch.load(\"models/belief-autoencoder-gumbel-128-512-32-FatzuhGa.pth\", map_location='cpu'))\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodels/belief-autoencoder-bc-64-128-16-H586q3U4.pth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/module.py:1604\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict)\u001B[0m\n\u001B[1;32m   1599\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   1600\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1601\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   1603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 1604\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1605\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   1606\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for BeliefAutoencoder:\n\tMissing key(s) in state_dict: \"lstm_encoder.weight_ih_l0\", \"lstm_encoder.weight_hh_l0\", \"lstm_encoder.bias_ih_l0\", \"lstm_encoder.bias_hh_l0\", \"lstm_encoder.weight_ih_l0_reverse\", \"lstm_encoder.weight_hh_l0_reverse\", \"lstm_encoder.bias_ih_l0_reverse\", \"lstm_encoder.bias_hh_l0_reverse\", \"lstm_decoder.weight_ih_l0\", \"lstm_decoder.weight_hh_l0\", \"lstm_decoder.bias_ih_l0\", \"lstm_decoder.bias_hh_l0\". \n\tUnexpected key(s) in state_dict: \"encoder.weight_ih_l0\", \"encoder.weight_hh_l0\", \"encoder.bias_ih_l0\", \"encoder.bias_hh_l0\", \"encoder.weight_ih_l0_reverse\", \"encoder.weight_hh_l0_reverse\", \"encoder.bias_ih_l0_reverse\", \"encoder.bias_hh_l0_reverse\", \"decoder.weight_ih_l0\", \"decoder.weight_hh_l0\", \"decoder.bias_ih_l0\", \"decoder.bias_hh_l0\", \"decoder.weight_ih_l0_reverse\", \"decoder.weight_hh_l0_reverse\", \"decoder.bias_ih_l0_reverse\", \"decoder.bias_hh_l0_reverse\". \n\tsize mismatch for sampling_input.weight: copying a param with shape torch.Size([16, 256]) from checkpoint, the shape in current model is torch.Size([16, 512]).\n\tsize mismatch for z_embedding.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n\tsize mismatch for z_embedding.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "from sources.model import BeliefAutoencoder, gumbel_softmax\n",
    "from sources.preprocessing import preprocessing, preprocess_sentence\n",
    "\n",
    "def set_seed(seed=20190827):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "preprocessed_data = preprocessing(\"data/dataset_sentence_level.csv\", device='cpu')\n",
    "vocab = preprocessed_data.vocab\n",
    "model = BeliefAutoencoder(emb_dim=64, h_dim=128, latent_dim=16, vocab=vocab, categorical_dim=1, activation='bc')\n",
    "\n",
    "#model.load_state_dict(torch.load(\"models/belief-autoencoder-bc-YP68fCFu.pth\", map_location='cpu'))\n",
    "#model.load_state_dict(torch.load(\"models/belief-autoencoder-gumbel-128-512-32-FatzuhGa.pth\", map_location='cpu'))\n",
    "model.load_state_dict(torch.load(\"models/belief-autoencoder-bc-64-128-16-H586q3U4.pth\", map_location='cpu'))\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = [\"you have an orange in your inventory\", \"you have an apple in your inventory\"]\n",
    "seq_lens = [len(s.split()) for s in sentences]\n",
    "max_len = max(seq_lens)\n",
    "vectorized = []\n",
    "for s in sentences:\n",
    "    idxs, tgt, seq_len = preprocess_sentence(s, vocab, max_len)\n",
    "    vectorized.append(idxs)\n",
    "tensorized = torch.tensor(vectorized)\n",
    "\n",
    "seq_len = torch.tensor(seq_lens)\n",
    "tensorized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ichida/anaconda3/envs/ml/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.],\n        [1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat, qy = model(x=tensorized, temperature=0.5)\n",
    "#qy = gumbel_softmax(qy, temperature=1e-20,latent_dim=30, categorical_dim=2, hard=True)\n",
    "qy_sig = F.sigmoid(qy)\n",
    "torch.round(qy_sig).squeeze(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent = torch.round(qy_sig).squeeze(-1)\n",
    "latent = latent.view(2, 1, 32)\n",
    "latent_vectors = latent.detach().cpu().numpy()\n",
    "for vector in latent_vectors:\n",
    "    plt.figure()\n",
    "    plt.imshow(vector, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'have', 'an', 'orange', 'in', 'your', 'inventory', '<EOS>', 'agent']\n",
      "[ 3  4 18 48 15 16 17  2 79]\n",
      "['a', 'apple', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>']\n",
      "[  5 156   2   2   2   2   2   2   2]\n"
     ]
    }
   ],
   "source": [
    "inverted_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "y_hat.argmax(dim=-1)\n",
    "idxs = y_hat.argmax(dim=-1).detach().cpu().numpy()\n",
    "#idx = model.inference(qy, 20)\n",
    "for idx in idxs:\n",
    "    print([inverted_vocab[i] for i in idx])\n",
    "    print(idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 60]' is invalid for input of size 32",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m next_word \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([vocab[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<SOS>\u001B[39m\u001B[38;5;124m'\u001B[39m]])\n\u001B[0;32m----> 2\u001B[0m z_emb \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mz_embedding(\u001B[43mqy\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m60\u001B[39;49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# z_emb será usado como h inicial do LSTM decoder\u001B[39;00m\n\u001B[1;32m      4\u001B[0m _, batch_size, hidden_len \u001B[38;5;241m=\u001B[39m z_emb\u001B[38;5;241m.\u001B[39msize()\n\u001B[1;32m      5\u001B[0m hidden \u001B[38;5;241m=\u001B[39m z_emb\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mint\u001B[39m(hidden_len \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m))  \u001B[38;5;66;03m# h_t\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: shape '[1, 60]' is invalid for input of size 32"
     ]
    }
   ],
   "source": [
    "next_word = torch.tensor([vocab['<SOS>']])\n",
    "z_emb = model.z_embedding(qy[0,:,:].view(1, 60)).unsqueeze(0)  # z_emb será usado como h inicial do LSTM decoder\n",
    "\n",
    "_, batch_size, hidden_len = z_emb.size()\n",
    "hidden = z_emb.view(2, 1, int(hidden_len / 2))  # h_t\n",
    "sentence = []\n",
    "next_word = next_word.unsqueeze(0)\n",
    "print(hidden.size(), next_word.size())\n",
    "for i in range(max_len):\n",
    "    sentence.append(next_word)\n",
    "    next_word_emb = model.embedding(next_word)\n",
    "    x, hidden = model.lstm_decoder(next_word_emb, hidden)\n",
    "    #x, _ = pad_packed_sequence(x)\n",
    "    x = model.output_layer(x)\n",
    "    next_word = x.argmax(dim=-1)\n",
    "s = torch.cat(sentence)\n",
    "s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "['<SOS>', '<PAD>', '<PAD>', '<PAD>', '3', '<PAD>', '<PAD>']"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inverted_vocab[i] for i in s.squeeze(-1).detach().numpy().tolist()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[8.2806e-04, 9.9941e-01],\n         [6.5952e-01, 3.7537e-01],\n         [1.3543e-02, 9.8676e-01],\n         [7.6780e-02, 9.3148e-01],\n         [9.9985e-01, 1.1610e-04],\n         [9.9946e-01, 2.8733e-04],\n         [9.9685e-01, 6.6426e-03],\n         [9.9788e-01, 5.5238e-03],\n         [9.9924e-01, 6.1472e-04],\n         [9.9751e-05, 9.9984e-01],\n         [7.9383e-07, 1.0000e+00],\n         [2.1500e-01, 7.1328e-01],\n         [9.1362e-03, 9.8036e-01],\n         [8.4709e-06, 9.9998e-01],\n         [9.5656e-01, 5.3405e-02],\n         [2.4508e-02, 9.5849e-01],\n         [1.8293e-02, 9.8606e-01],\n         [9.7044e-01, 1.9000e-02],\n         [8.6453e-02, 8.4050e-01],\n         [9.9999e-01, 1.7152e-05],\n         [4.0549e-02, 9.8730e-01],\n         [1.0000e+00, 3.3609e-07],\n         [1.2495e-02, 9.7839e-01],\n         [2.1059e-01, 7.9157e-01],\n         [3.6606e-05, 9.9985e-01],\n         [1.0368e-04, 9.9987e-01],\n         [8.2068e-03, 9.9318e-01],\n         [1.0000e+00, 5.1429e-07],\n         [1.1812e-06, 1.0000e+00],\n         [7.9596e-01, 1.5260e-01]]),\n tensor([[0.0314, 0.9670],\n         [0.9863, 0.0138],\n         [0.6176, 0.4158],\n         [0.6613, 0.2586],\n         [0.0011, 0.9990],\n         [0.9964, 0.0035],\n         [0.2040, 0.8675],\n         [0.6814, 0.2242],\n         [0.9967, 0.0034],\n         [0.3212, 0.5846],\n         [0.5895, 0.3386],\n         [0.0951, 0.8887],\n         [0.4411, 0.4725],\n         [0.9933, 0.0101],\n         [0.2036, 0.7176],\n         [0.5863, 0.3589],\n         [0.0013, 0.9988],\n         [0.8324, 0.1257],\n         [0.6578, 0.4205],\n         [0.9878, 0.0085],\n         [0.0479, 0.9585],\n         [0.9961, 0.0040],\n         [0.0358, 0.9769],\n         [0.1588, 0.9025],\n         [0.6934, 0.2413],\n         [0.0294, 0.9771],\n         [0.8337, 0.1701],\n         [0.0449, 0.9365],\n         [0.1169, 0.9013],\n         [0.3528, 0.3961]]))"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qy[0,:,:], qy[1,:,:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}