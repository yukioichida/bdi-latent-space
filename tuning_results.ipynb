{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['train_id', 'emb_dim', 'h_dim', 'latent_dim', 'categorical_dim',\n       'batch_size', 'save_model', 'initial_temp', 'min_temp', 'epochs',\n       'anneal_rate', 'activation', 'model_name', 'current_epoch',\n       'train_loss', 'kld', 'recon_loss'],\n      dtype='object')"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "result_df = pd.read_csv(\"train_results/tunning_results_ALRk76VV.csv\")\n",
    "result_df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "    emb_dim  h_dim  latent_dim  categorical_dim activation  train_loss\n31      512    512          32                2     gumbel   96.043434\n32      512   1024          16                2         bc  103.721061\n27      512    256          32                2     gumbel  104.189795\n16      256    512          16                2         bc  104.905645\n20      256   1024          16                2         bc  104.951406\n28      512    512          16                2         bc  105.088224\n33      512   1024          16                2     gumbel  105.464633\n5       128    512          16                2     gumbel  106.248179\n4       128    512          16                2         bc  106.424144\n8       128   1024          16                2         bc  106.602033\n29      512    512          16                2     gumbel  106.710121\n18      256    512          32                2         bc  106.749067\n10      128   1024          32                2         bc  106.884060\n21      256   1024          16                2     gumbel  106.982867\n13      256    256          16                2     gumbel  106.991443\n34      512   1024          32                2         bc  107.337648\n24      512    256          16                2         bc  107.480092\n17      256    512          16                2     gumbel  107.661122\n22      256   1024          32                2         bc  107.809865\n9       128   1024          16                2     gumbel  107.958055\n25      512    256          16                2     gumbel  108.113178\n30      512    512          32                2         bc  108.250737\n26      512    256          32                2         bc  108.267397\n12      256    256          16                2         bc  108.280732\n0       128    256          16                2         bc  108.349851\n6       128    512          32                2         bc  108.350701\n3       128    256          32                2     gumbel  108.357887\n35      512   1024          32                2     gumbel  108.363004\n14      256    256          32                2         bc  108.373603\n11      128   1024          32                2     gumbel  108.432273\n15      256    256          32                2     gumbel  108.557175\n7       128    512          32                2     gumbel  108.681959\n23      256   1024          32                2     gumbel  108.770987\n19      256    512          32                2     gumbel  108.823281\n2       128    256          32                2         bc  108.980891\n1       128    256          16                2     gumbel  109.558643",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emb_dim</th>\n      <th>h_dim</th>\n      <th>latent_dim</th>\n      <th>categorical_dim</th>\n      <th>activation</th>\n      <th>train_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>31</th>\n      <td>512</td>\n      <td>512</td>\n      <td>32</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>96.043434</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>512</td>\n      <td>1024</td>\n      <td>16</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>103.721061</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>512</td>\n      <td>256</td>\n      <td>32</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>104.189795</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>256</td>\n      <td>512</td>\n      <td>16</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>104.905645</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>256</td>\n      <td>1024</td>\n      <td>16</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>104.951406</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>512</td>\n      <td>512</td>\n      <td>16</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>105.088224</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>512</td>\n      <td>1024</td>\n      <td>16</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>105.464633</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>128</td>\n      <td>512</td>\n      <td>16</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>106.248179</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>128</td>\n      <td>512</td>\n      <td>16</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>106.424144</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>128</td>\n      <td>1024</td>\n      <td>16</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>106.602033</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>512</td>\n      <td>512</td>\n      <td>16</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>106.710121</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>256</td>\n      <td>512</td>\n      <td>32</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>106.749067</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>128</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>106.884060</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>256</td>\n      <td>1024</td>\n      <td>16</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>106.982867</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>256</td>\n      <td>256</td>\n      <td>16</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>106.991443</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>107.337648</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>512</td>\n      <td>256</td>\n      <td>16</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>107.480092</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>256</td>\n      <td>512</td>\n      <td>16</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>107.661122</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>256</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>107.809865</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>128</td>\n      <td>1024</td>\n      <td>16</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>107.958055</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>512</td>\n      <td>256</td>\n      <td>16</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>108.113178</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>512</td>\n      <td>512</td>\n      <td>32</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>108.250737</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>512</td>\n      <td>256</td>\n      <td>32</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>108.267397</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>256</td>\n      <td>256</td>\n      <td>16</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>108.280732</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>128</td>\n      <td>256</td>\n      <td>16</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>108.349851</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>128</td>\n      <td>512</td>\n      <td>32</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>108.350701</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>128</td>\n      <td>256</td>\n      <td>32</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>108.357887</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>108.363004</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>256</td>\n      <td>256</td>\n      <td>32</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>108.373603</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>128</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>108.432273</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>256</td>\n      <td>256</td>\n      <td>32</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>108.557175</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>128</td>\n      <td>512</td>\n      <td>32</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>108.681959</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>256</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>108.770987</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>256</td>\n      <td>512</td>\n      <td>32</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>108.823281</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>128</td>\n      <td>256</td>\n      <td>32</td>\n      <td>2</td>\n      <td>bc</td>\n      <td>108.980891</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>128</td>\n      <td>256</td>\n      <td>16</td>\n      <td>2</td>\n      <td>gumbel</td>\n      <td>109.558643</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['emb_dim', 'h_dim', 'latent_dim', 'categorical_dim', 'activation']\n",
    "\n",
    "grouped_df = result_df.groupby(columns).agg({'train_loss': 'min'})\n",
    "grouped_df.reset_index().sort_values([\"train_loss\", \"latent_dim\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "        train_id  emb_dim  h_dim  latent_dim  categorical_dim  batch_size  \\\n1199  tunning_23      512   1024          32                2         128   \n1198  tunning_23      512   1024          32                2         128   \n1197  tunning_23      512   1024          32                2         128   \n1196  tunning_23      512   1024          32                2         128   \n1195  tunning_23      512   1024          32                2         128   \n1194  tunning_23      512   1024          32                2         128   \n1193  tunning_23      512   1024          32                2         128   \n1192  tunning_23      512   1024          32                2         128   \n1191  tunning_23      512   1024          32                2         128   \n1190  tunning_23      512   1024          32                2         128   \n1189  tunning_23      512   1024          32                2         128   \n1188  tunning_23      512   1024          32                2         128   \n1187  tunning_23      512   1024          32                2         128   \n1186  tunning_23      512   1024          32                2         128   \n1185  tunning_23      512   1024          32                2         128   \n1184  tunning_23      512   1024          32                2         128   \n1183  tunning_23      512   1024          32                2         128   \n1182  tunning_23      512   1024          32                2         128   \n1181  tunning_23      512   1024          32                2         128   \n1180  tunning_23      512   1024          32                2         128   \n1179  tunning_23      512   1024          32                2         128   \n1178  tunning_23      512   1024          32                2         128   \n1177  tunning_23      512   1024          32                2         128   \n1176  tunning_23      512   1024          32                2         128   \n1175  tunning_23      512   1024          32                2         128   \n1174  tunning_23      512   1024          32                2         128   \n1173  tunning_23      512   1024          32                2         128   \n1172  tunning_23      512   1024          32                2         128   \n1171  tunning_23      512   1024          32                2         128   \n1170  tunning_23      512   1024          32                2         128   \n1169  tunning_23      512   1024          32                2         128   \n1168  tunning_23      512   1024          32                2         128   \n1167  tunning_23      512   1024          32                2         128   \n1166  tunning_23      512   1024          32                2         128   \n1165  tunning_23      512   1024          32                2         128   \n1164  tunning_23      512   1024          32                2         128   \n1163  tunning_23      512   1024          32                2         128   \n1162  tunning_23      512   1024          32                2         128   \n1161  tunning_23      512   1024          32                2         128   \n1160  tunning_23      512   1024          32                2         128   \n1159  tunning_23      512   1024          32                2         128   \n1158  tunning_23      512   1024          32                2         128   \n1157  tunning_23      512   1024          32                2         128   \n1156  tunning_23      512   1024          32                2         128   \n1155  tunning_23      512   1024          32                2         128   \n1154  tunning_23      512   1024          32                2         128   \n1153  tunning_23      512   1024          32                2         128   \n1152  tunning_23      512   1024          32                2         128   \n1151  tunning_23      512   1024          32                2         128   \n1150  tunning_23      512   1024          32                2         128   \n\n      save_model  initial_temp  min_temp  epochs  anneal_rate activation  \\\n1199       False           1.0       0.5      50      0.00003         bc   \n1198       False           1.0       0.5      50      0.00003         bc   \n1197       False           1.0       0.5      50      0.00003         bc   \n1196       False           1.0       0.5      50      0.00003         bc   \n1195       False           1.0       0.5      50      0.00003         bc   \n1194       False           1.0       0.5      50      0.00003         bc   \n1193       False           1.0       0.5      50      0.00003         bc   \n1192       False           1.0       0.5      50      0.00003         bc   \n1191       False           1.0       0.5      50      0.00003         bc   \n1190       False           1.0       0.5      50      0.00003         bc   \n1189       False           1.0       0.5      50      0.00003         bc   \n1188       False           1.0       0.5      50      0.00003         bc   \n1187       False           1.0       0.5      50      0.00003         bc   \n1186       False           1.0       0.5      50      0.00003         bc   \n1185       False           1.0       0.5      50      0.00003         bc   \n1184       False           1.0       0.5      50      0.00003         bc   \n1183       False           1.0       0.5      50      0.00003         bc   \n1182       False           1.0       0.5      50      0.00003         bc   \n1181       False           1.0       0.5      50      0.00003         bc   \n1180       False           1.0       0.5      50      0.00003         bc   \n1179       False           1.0       0.5      50      0.00003         bc   \n1178       False           1.0       0.5      50      0.00003         bc   \n1177       False           1.0       0.5      50      0.00003         bc   \n1176       False           1.0       0.5      50      0.00003         bc   \n1175       False           1.0       0.5      50      0.00003         bc   \n1174       False           1.0       0.5      50      0.00003         bc   \n1173       False           1.0       0.5      50      0.00003         bc   \n1172       False           1.0       0.5      50      0.00003         bc   \n1171       False           1.0       0.5      50      0.00003         bc   \n1170       False           1.0       0.5      50      0.00003         bc   \n1169       False           1.0       0.5      50      0.00003         bc   \n1168       False           1.0       0.5      50      0.00003         bc   \n1167       False           1.0       0.5      50      0.00003         bc   \n1166       False           1.0       0.5      50      0.00003         bc   \n1165       False           1.0       0.5      50      0.00003         bc   \n1164       False           1.0       0.5      50      0.00003         bc   \n1163       False           1.0       0.5      50      0.00003         bc   \n1162       False           1.0       0.5      50      0.00003         bc   \n1161       False           1.0       0.5      50      0.00003         bc   \n1160       False           1.0       0.5      50      0.00003         bc   \n1159       False           1.0       0.5      50      0.00003         bc   \n1158       False           1.0       0.5      50      0.00003         bc   \n1157       False           1.0       0.5      50      0.00003         bc   \n1156       False           1.0       0.5      50      0.00003         bc   \n1155       False           1.0       0.5      50      0.00003         bc   \n1154       False           1.0       0.5      50      0.00003         bc   \n1153       False           1.0       0.5      50      0.00003         bc   \n1152       False           1.0       0.5      50      0.00003         bc   \n1151       False           1.0       0.5      50      0.00003         bc   \n1150       False           1.0       0.5      50      0.00003         bc   \n\n               model_name  current_epoch  train_loss       kld  recon_loss  \n1199  tunning_autoencoder             49  130.578981  0.267589  130.311392  \n1198  tunning_autoencoder             48  126.199872  0.174633  126.025239  \n1197  tunning_autoencoder             47  123.515516  0.247876  123.267640  \n1196  tunning_autoencoder             46  125.322314  2.706965  122.615349  \n1195  tunning_autoencoder             45  127.919992  0.237781  127.682211  \n1194  tunning_autoencoder             44  122.568652  0.237194  122.331457  \n1193  tunning_autoencoder             43  120.888105  0.158712  120.729393  \n1192  tunning_autoencoder             42  128.954420  0.190629  128.763792  \n1191  tunning_autoencoder             41  122.371136  0.163821  122.207315  \n1190  tunning_autoencoder             40  118.317441  0.172461  118.144980  \n1189  tunning_autoencoder             39  119.650659  0.159535  119.491125  \n1188  tunning_autoencoder             38  118.711201  0.196039  118.515163  \n1187  tunning_autoencoder             37  121.626426  2.341670  119.284755  \n1186  tunning_autoencoder             36  117.206146  0.015583  117.190563  \n1185  tunning_autoencoder             35  114.177245  0.015016  114.162229  \n1184  tunning_autoencoder             34  114.596610  0.013240  114.583370  \n1183  tunning_autoencoder             33  111.692924  0.019028  111.673896  \n1182  tunning_autoencoder             32  111.019879  0.009834  111.010045  \n1181  tunning_autoencoder             31  109.518048  0.010250  109.507798  \n1180  tunning_autoencoder             30  108.722634  0.007268  108.715366  \n1179  tunning_autoencoder             29  108.957449  0.009538  108.947911  \n1178  tunning_autoencoder             28  108.683372  0.021956  108.661417  \n1177  tunning_autoencoder             27  107.959952  0.004536  107.955416  \n1176  tunning_autoencoder             26  107.887781  0.003468  107.884314  \n1175  tunning_autoencoder             25  107.337648  0.003331  107.334318  \n1174  tunning_autoencoder             24  107.526483  0.003471  107.523011  \n1173  tunning_autoencoder             23  108.880200  0.055368  108.824832  \n1172  tunning_autoencoder             22  107.923056  0.002671  107.920384  \n1171  tunning_autoencoder             21  108.138443  0.001780  108.136664  \n1170  tunning_autoencoder             20  108.465159  0.002342  108.462817  \n1169  tunning_autoencoder             19  109.281877  0.003133  109.278744  \n1168  tunning_autoencoder             18  109.594042  0.003750  109.590292  \n1167  tunning_autoencoder             17  110.494102  0.005158  110.488944  \n1166  tunning_autoencoder             16  110.501376  0.003838  110.497538  \n1165  tunning_autoencoder             15  110.589192  0.004351  110.584841  \n1164  tunning_autoencoder             14  111.035246  0.004125  111.031121  \n1163  tunning_autoencoder             13  111.742772  0.004070  111.738702  \n1162  tunning_autoencoder             12  112.247437  0.006869  112.240568  \n1161  tunning_autoencoder             11  112.904903  0.004095  112.900809  \n1160  tunning_autoencoder             10  113.336782  0.008198  113.328584  \n1159  tunning_autoencoder              9  113.896737  0.006627  113.890110  \n1158  tunning_autoencoder              8  114.411467  0.005892  114.405575  \n1157  tunning_autoencoder              7  114.900621  0.005263  114.895359  \n1156  tunning_autoencoder              6  115.246795  0.004181  115.242614  \n1155  tunning_autoencoder              5  115.658354  0.003194  115.655160  \n1154  tunning_autoencoder              4  116.605778  0.006892  116.598886  \n1153  tunning_autoencoder              3  117.227607  0.004137  117.223470  \n1152  tunning_autoencoder              2  117.997239  0.005074  117.992165  \n1151  tunning_autoencoder              1  119.667100  0.008322  119.658777  \n1150  tunning_autoencoder              0  124.077818  0.005258  124.072560  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>train_id</th>\n      <th>emb_dim</th>\n      <th>h_dim</th>\n      <th>latent_dim</th>\n      <th>categorical_dim</th>\n      <th>batch_size</th>\n      <th>save_model</th>\n      <th>initial_temp</th>\n      <th>min_temp</th>\n      <th>epochs</th>\n      <th>anneal_rate</th>\n      <th>activation</th>\n      <th>model_name</th>\n      <th>current_epoch</th>\n      <th>train_loss</th>\n      <th>kld</th>\n      <th>recon_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1199</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>49</td>\n      <td>130.578981</td>\n      <td>0.267589</td>\n      <td>130.311392</td>\n    </tr>\n    <tr>\n      <th>1198</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>48</td>\n      <td>126.199872</td>\n      <td>0.174633</td>\n      <td>126.025239</td>\n    </tr>\n    <tr>\n      <th>1197</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>47</td>\n      <td>123.515516</td>\n      <td>0.247876</td>\n      <td>123.267640</td>\n    </tr>\n    <tr>\n      <th>1196</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>46</td>\n      <td>125.322314</td>\n      <td>2.706965</td>\n      <td>122.615349</td>\n    </tr>\n    <tr>\n      <th>1195</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>45</td>\n      <td>127.919992</td>\n      <td>0.237781</td>\n      <td>127.682211</td>\n    </tr>\n    <tr>\n      <th>1194</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>44</td>\n      <td>122.568652</td>\n      <td>0.237194</td>\n      <td>122.331457</td>\n    </tr>\n    <tr>\n      <th>1193</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>43</td>\n      <td>120.888105</td>\n      <td>0.158712</td>\n      <td>120.729393</td>\n    </tr>\n    <tr>\n      <th>1192</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>42</td>\n      <td>128.954420</td>\n      <td>0.190629</td>\n      <td>128.763792</td>\n    </tr>\n    <tr>\n      <th>1191</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>41</td>\n      <td>122.371136</td>\n      <td>0.163821</td>\n      <td>122.207315</td>\n    </tr>\n    <tr>\n      <th>1190</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>40</td>\n      <td>118.317441</td>\n      <td>0.172461</td>\n      <td>118.144980</td>\n    </tr>\n    <tr>\n      <th>1189</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>39</td>\n      <td>119.650659</td>\n      <td>0.159535</td>\n      <td>119.491125</td>\n    </tr>\n    <tr>\n      <th>1188</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>38</td>\n      <td>118.711201</td>\n      <td>0.196039</td>\n      <td>118.515163</td>\n    </tr>\n    <tr>\n      <th>1187</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>37</td>\n      <td>121.626426</td>\n      <td>2.341670</td>\n      <td>119.284755</td>\n    </tr>\n    <tr>\n      <th>1186</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>36</td>\n      <td>117.206146</td>\n      <td>0.015583</td>\n      <td>117.190563</td>\n    </tr>\n    <tr>\n      <th>1185</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>35</td>\n      <td>114.177245</td>\n      <td>0.015016</td>\n      <td>114.162229</td>\n    </tr>\n    <tr>\n      <th>1184</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>34</td>\n      <td>114.596610</td>\n      <td>0.013240</td>\n      <td>114.583370</td>\n    </tr>\n    <tr>\n      <th>1183</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>33</td>\n      <td>111.692924</td>\n      <td>0.019028</td>\n      <td>111.673896</td>\n    </tr>\n    <tr>\n      <th>1182</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>32</td>\n      <td>111.019879</td>\n      <td>0.009834</td>\n      <td>111.010045</td>\n    </tr>\n    <tr>\n      <th>1181</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>31</td>\n      <td>109.518048</td>\n      <td>0.010250</td>\n      <td>109.507798</td>\n    </tr>\n    <tr>\n      <th>1180</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>30</td>\n      <td>108.722634</td>\n      <td>0.007268</td>\n      <td>108.715366</td>\n    </tr>\n    <tr>\n      <th>1179</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>29</td>\n      <td>108.957449</td>\n      <td>0.009538</td>\n      <td>108.947911</td>\n    </tr>\n    <tr>\n      <th>1178</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>28</td>\n      <td>108.683372</td>\n      <td>0.021956</td>\n      <td>108.661417</td>\n    </tr>\n    <tr>\n      <th>1177</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>27</td>\n      <td>107.959952</td>\n      <td>0.004536</td>\n      <td>107.955416</td>\n    </tr>\n    <tr>\n      <th>1176</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>26</td>\n      <td>107.887781</td>\n      <td>0.003468</td>\n      <td>107.884314</td>\n    </tr>\n    <tr>\n      <th>1175</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>25</td>\n      <td>107.337648</td>\n      <td>0.003331</td>\n      <td>107.334318</td>\n    </tr>\n    <tr>\n      <th>1174</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>24</td>\n      <td>107.526483</td>\n      <td>0.003471</td>\n      <td>107.523011</td>\n    </tr>\n    <tr>\n      <th>1173</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>23</td>\n      <td>108.880200</td>\n      <td>0.055368</td>\n      <td>108.824832</td>\n    </tr>\n    <tr>\n      <th>1172</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>22</td>\n      <td>107.923056</td>\n      <td>0.002671</td>\n      <td>107.920384</td>\n    </tr>\n    <tr>\n      <th>1171</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>21</td>\n      <td>108.138443</td>\n      <td>0.001780</td>\n      <td>108.136664</td>\n    </tr>\n    <tr>\n      <th>1170</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>20</td>\n      <td>108.465159</td>\n      <td>0.002342</td>\n      <td>108.462817</td>\n    </tr>\n    <tr>\n      <th>1169</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>19</td>\n      <td>109.281877</td>\n      <td>0.003133</td>\n      <td>109.278744</td>\n    </tr>\n    <tr>\n      <th>1168</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>18</td>\n      <td>109.594042</td>\n      <td>0.003750</td>\n      <td>109.590292</td>\n    </tr>\n    <tr>\n      <th>1167</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>17</td>\n      <td>110.494102</td>\n      <td>0.005158</td>\n      <td>110.488944</td>\n    </tr>\n    <tr>\n      <th>1166</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>16</td>\n      <td>110.501376</td>\n      <td>0.003838</td>\n      <td>110.497538</td>\n    </tr>\n    <tr>\n      <th>1165</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>15</td>\n      <td>110.589192</td>\n      <td>0.004351</td>\n      <td>110.584841</td>\n    </tr>\n    <tr>\n      <th>1164</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>14</td>\n      <td>111.035246</td>\n      <td>0.004125</td>\n      <td>111.031121</td>\n    </tr>\n    <tr>\n      <th>1163</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>13</td>\n      <td>111.742772</td>\n      <td>0.004070</td>\n      <td>111.738702</td>\n    </tr>\n    <tr>\n      <th>1162</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>12</td>\n      <td>112.247437</td>\n      <td>0.006869</td>\n      <td>112.240568</td>\n    </tr>\n    <tr>\n      <th>1161</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>11</td>\n      <td>112.904903</td>\n      <td>0.004095</td>\n      <td>112.900809</td>\n    </tr>\n    <tr>\n      <th>1160</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>10</td>\n      <td>113.336782</td>\n      <td>0.008198</td>\n      <td>113.328584</td>\n    </tr>\n    <tr>\n      <th>1159</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>9</td>\n      <td>113.896737</td>\n      <td>0.006627</td>\n      <td>113.890110</td>\n    </tr>\n    <tr>\n      <th>1158</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>8</td>\n      <td>114.411467</td>\n      <td>0.005892</td>\n      <td>114.405575</td>\n    </tr>\n    <tr>\n      <th>1157</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>7</td>\n      <td>114.900621</td>\n      <td>0.005263</td>\n      <td>114.895359</td>\n    </tr>\n    <tr>\n      <th>1156</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>6</td>\n      <td>115.246795</td>\n      <td>0.004181</td>\n      <td>115.242614</td>\n    </tr>\n    <tr>\n      <th>1155</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>5</td>\n      <td>115.658354</td>\n      <td>0.003194</td>\n      <td>115.655160</td>\n    </tr>\n    <tr>\n      <th>1154</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>4</td>\n      <td>116.605778</td>\n      <td>0.006892</td>\n      <td>116.598886</td>\n    </tr>\n    <tr>\n      <th>1153</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>3</td>\n      <td>117.227607</td>\n      <td>0.004137</td>\n      <td>117.223470</td>\n    </tr>\n    <tr>\n      <th>1152</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>2</td>\n      <td>117.997239</td>\n      <td>0.005074</td>\n      <td>117.992165</td>\n    </tr>\n    <tr>\n      <th>1151</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>1</td>\n      <td>119.667100</td>\n      <td>0.008322</td>\n      <td>119.658777</td>\n    </tr>\n    <tr>\n      <th>1150</th>\n      <td>tunning_23</td>\n      <td>512</td>\n      <td>1024</td>\n      <td>32</td>\n      <td>2</td>\n      <td>128</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>50</td>\n      <td>0.00003</td>\n      <td>bc</td>\n      <td>tunning_autoencoder</td>\n      <td>0</td>\n      <td>124.077818</td>\n      <td>0.005258</td>\n      <td>124.072560</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 512\n",
    "h_dim = 1024\n",
    "latent_dim = 32\n",
    "activation = 'bc'\n",
    "\n",
    "df = result_df[(result_df['emb_dim'] == emb_dim) & (result_df['h_dim'] == h_dim) & (result_df['latent_dim'] == latent_dim) & (result_df['activation'] == activation)]\n",
    "df.sort_values('current_epoch', ascending=False)#[['current_epoch', 'train_loss']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns = ['emb_dim', 'h_dim', 'latent_dim', 'categorical_dim']\n",
    "bc_df = result_df[result_df['activation'] == 'bc']\n",
    "\n",
    "num_groups = len(bc_df.groupby(columns))\n",
    "fig, axs = plt.subplots(num_groups, 1, figsize=(40, 200))\n",
    "i = 0\n",
    "for group, group_df in bc_df.groupby(columns):\n",
    "\n",
    "    plot_df = group_df.sort_values('current_epoch').reset_index(drop=True)\n",
    "    plot_df[['train_loss', 'recon_loss', 'kld']].plot(label='train_loss', ax = axs[i])\n",
    "    #axs[i].plot(plot_df['train_loss'], label='train_loss', color='green')\n",
    "    #axs[i].plot(plot_df['train_loss'], label='recon_loss', color='steelblue')\n",
    "    #axs[i].plot(plot_df['train_loss'], label='kld', color='purple')\n",
    "    title = f\"emb_dim = {group[0]} - h_dim = {group[1]} - latent_dim = {group[2]} - cat dim {group[3]}\"\n",
    "    axs[i].set_title(title)\n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "    #plot_df['train_loss'].plot()\n",
    "    #plot_df['recon_loss'].plot()\n",
    "    #plot_df['kld'].plot()\n",
    "#fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "      current_epoch         kld  recon_loss  train_loss  latent_dim  h_dim  \\\n1078             28    0.004378  103.716683  103.721061          16   1024   \n1077             27    0.003854  104.413706  104.417560          16   1024   \n1079             29    0.005892  104.592248  104.598141          16   1024   \n1080             30    0.011946  104.694702  104.706648          16   1024   \n1495             45    0.003690  104.901955  104.905645          16    512   \n...             ...         ...         ...         ...         ...    ...   \n1784             34  266.306042  110.273763  376.579805          32   1024   \n1783             33  266.454143  110.606246  377.060390          32   1024   \n1775             25  275.629832  107.791613  383.421446          32   1024   \n1698             48  434.679469  113.367176  548.046645          16   1024   \n1082             32  584.847675  105.452858  690.300533          16   1024   \n\n      emb_dim  \n1078      512  \n1077      512  \n1079      512  \n1080      512  \n1495      256  \n...       ...  \n1784      256  \n1783      256  \n1775      256  \n1698      256  \n1082      512  \n\n[900 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>current_epoch</th>\n      <th>kld</th>\n      <th>recon_loss</th>\n      <th>train_loss</th>\n      <th>latent_dim</th>\n      <th>h_dim</th>\n      <th>emb_dim</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1078</th>\n      <td>28</td>\n      <td>0.004378</td>\n      <td>103.716683</td>\n      <td>103.721061</td>\n      <td>16</td>\n      <td>1024</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>1077</th>\n      <td>27</td>\n      <td>0.003854</td>\n      <td>104.413706</td>\n      <td>104.417560</td>\n      <td>16</td>\n      <td>1024</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>1079</th>\n      <td>29</td>\n      <td>0.005892</td>\n      <td>104.592248</td>\n      <td>104.598141</td>\n      <td>16</td>\n      <td>1024</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>1080</th>\n      <td>30</td>\n      <td>0.011946</td>\n      <td>104.694702</td>\n      <td>104.706648</td>\n      <td>16</td>\n      <td>1024</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>45</td>\n      <td>0.003690</td>\n      <td>104.901955</td>\n      <td>104.905645</td>\n      <td>16</td>\n      <td>512</td>\n      <td>256</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1784</th>\n      <td>34</td>\n      <td>266.306042</td>\n      <td>110.273763</td>\n      <td>376.579805</td>\n      <td>32</td>\n      <td>1024</td>\n      <td>256</td>\n    </tr>\n    <tr>\n      <th>1783</th>\n      <td>33</td>\n      <td>266.454143</td>\n      <td>110.606246</td>\n      <td>377.060390</td>\n      <td>32</td>\n      <td>1024</td>\n      <td>256</td>\n    </tr>\n    <tr>\n      <th>1775</th>\n      <td>25</td>\n      <td>275.629832</td>\n      <td>107.791613</td>\n      <td>383.421446</td>\n      <td>32</td>\n      <td>1024</td>\n      <td>256</td>\n    </tr>\n    <tr>\n      <th>1698</th>\n      <td>48</td>\n      <td>434.679469</td>\n      <td>113.367176</td>\n      <td>548.046645</td>\n      <td>16</td>\n      <td>1024</td>\n      <td>256</td>\n    </tr>\n    <tr>\n      <th>1082</th>\n      <td>32</td>\n      <td>584.847675</td>\n      <td>105.452858</td>\n      <td>690.300533</td>\n      <td>16</td>\n      <td>1024</td>\n      <td>512</td>\n    </tr>\n  </tbody>\n</table>\n<p>900 rows  7 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_df.sort_values(\"train_loss\")[['current_epoch', 'kld', 'recon_loss', 'train_loss', 'latent_dim', 'h_dim', 'emb_dim']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-34-67fc583deffe>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;34m','\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m512\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1024\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m: sequence item 0: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "ti"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
